# 里程碑4️⃣：可观测服务之监控

<div class="image-container">
    <img src="./docs/im/images/230.png" alt="图片30-1" title="图片30-1" >
    <span class="image-title">图 30-1 </span>
</div>

## 可观测性

在2020年9月，CNCF发布的一份以observability（可观测性）为主题的最终用户技术雷达（ [End User Technology Radar](https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/)）中提到，在2020年8月期间，最终用户社区（the members of the End User Community）的成员被问及他们评估、试验并随后采用了哪些可观察性解决方案。总共对283个数据点进行排序和审查以确定最终位置。如下所示：

<div class="image-container">
    <img src="./docs/im/images/231.png" alt="图片30-2" title="图片30-2" >
    <span class="image-title">图 30-2 </span>
</div>

其中，被提及的解决方案会处于三个不同的级别：

* ADOPT（采用）：指CNCF最终用户社区明确推荐这项技术。它已经在许多团队中长期使用，并被证明是稳定且有用的。
* TRIAL（试用）：CNCF最终用户社区已成功使用它，我们建议您仔细研究该技术。
* ASSESS（评估）：CNCF 最终用户社区已经尝试过，我们发现它很有希望。当您对项目中的技术有特定需求时，我们建议您查看这些项目。

同时，在这次发布的主题中，得出了三个有趣的结论：

1.最常用的工具是开源的：获得最多“ADOPT”票的三个工具（Prometheus、Grafana、Elastic）和获得总票数最多的五个工具（Prometheus、Grafana、Elastic、Jaeger、OpenTelemetry）都是开源的。
> The most commonly adopted tools are open source.  The three tools that received the most “Adopt” votes (Prometheus, Grafana, Elastic) and the five tools that received the most total votes (Prometheus, Grafana, Elastic, Jaeger, OpenTelemetry) are all open source.


2.可观察性空间没有整合：许多公司正在使用多种工具：一半的公司使用 5 种或更多的工具，其中三分之一的公司有使用 10 多种工具的经验。
> There’s no consolidation in the observability space.  Many companies are using multiple tools: Half of the companies are using 5 or more tools, and a third of them had experience with 10+ tools.


3.Prometheus 和 Grafana 经常一起使用: 三分之二的受访者同时使用这两种工具。
> Prometheus and Grafana are frequently used together.  Two-thirds of the respondents are using these two tools in tandem.

可以看出，随着部署日益复杂，越来越多的公司意识到云原生可观测性的重要意义，也就越来越倾向于在部署过程中使用到了监控、日志和追踪。

* 监控：通过收集系统服务的一些指标（metrics）来“实时”监控系统状态。比如从系统层面有CPU、MEM等使用率指标；而在HTTP服务层面则有QPS、RT(响应时间)等指标。
* 日志：随着系统架构越来越复杂，程序运行过程中生成的海量日志分布在不同的服务节点，导致日志的收集、查询就成了一个问题，相应的解决方案应运而生。
* 追踪：在微服务，一次请求可能会涉及到多个服务的调用，因此当出现问题时，很难从错综复杂的服务调用网络中找到问题根源，这时就需要一套分布式链路追踪系统，它提供调用链路还原、链路拓扑、服务依赖分析等功能，帮助开发者快速定位问题。

而其中典型的解决方案代表如下：

* 监控：Prometheus
* 日志：Elastic
* 追踪：Jaeger

其中链路追踪服务Jaeger使用golang语言开发，因此在golang生态中使用较多，另还有zipkin、skywalking等也是使用非常多的链路追踪解决方案。不过从功能性上来讲，这些技术方案通常聚焦于自己的核心功能，导致无法形成对用户非常友好的体验，也因此导致上面结论中的第2、3点的出现，也就是要组合使用。比如Prometheus虽然是非常好的度量解决方案，但是它提供的可视化能力非常弱，在实际情况下通常要结合Grafana使用。

<div class="image-container">
    <img src="./docs/im/images/232.png" alt="图片30-3" title="图片30-3" >
    <span class="image-title">图 30-3 </span>
</div>

回到kim项目中，由于日志与追踪更多是运维相关的工作，因此在这里就不再深入。在本章节中，我们通过集成Prometheus，介绍如何实现服务的监控。

## Prometheus简介

[Prometheus](https://github.com/prometheus) 是由前 Google 工程师从 2012 年开始在 Soundcloud 以开源软件的形式进行研发的系统监控和告警工具包。

### 架构介绍

Prometheus 的整体架构以及生态系统组件如下图所示：

<div class="image-container">
    <img src="./docs/im/images/233.png" alt="图片30-4" title="图片30-4" >
    <span class="image-title">图 30-4 </span>
</div>

Prometheus Server 直接从监控目标中或者间接通过推送网关来拉取监控指标，它在本地存储所有抓取到的样本数据，并对此数据执行一系列规则，以汇总和记录现有数据的新时间序列或生成告警。可以通过Grafana或者其他工具来实现监控数据的可视化。

### 构成组件

Prometheus 生态系统由多个组件组成，其中有许多组件是可选的：

* Prometheus Server：作为服务端，用来存储时间序列数据。
* Client Library：用来检测应用程序代码。
* Pushgateway： 用于支持临时任务的推送网关。
* Exporter: 用来监控特殊目标，并向Prometheus提供标准格式的监控样本数据。
* Alartmanager：用来处理告警。

在使用Prometheus的过程中，通常有两种方式把指标提供给Prometheus Server：

1.部署特殊的Exporter。比如在一台服务器上部署一个[node_exporter](https://github.com/prometheus/node_exporter)，它默认提供一个9100端口的web服务，就可以把此节点的CPU、内存、网络等相关指标Metrics提供出来。下图就是部署一个node_exporter服务之后，它提供的/metrics 站点。

<div class="image-container">
    <img src="./docs/im/images/234.png" alt="图片30-5" title="图片30-5" >
    <span class="image-title">图 30-5 </span>
</div>

2.项目代码中集成Prometheus客户端Library。除了Library会提供出默认的metrics之外，还可以通过Library提供的接口编码实现业务自定义Metrics。下图就是网关与逻辑服务提供metrics之后的监控示例。

<div class="image-container">
    <img src="./docs/im/images/235.png" alt="图片30-6" title="图片30-6" >
    <span class="image-title">图 30-6 </span>
</div>

### 指标类型

Prometheus 的客户端库中提供了四种核心的指标类型。

* Counter：计数器。
* Gauge：仪表盘。

前两种指标类型比较简单。Counter类型代表一种样本数据单调递增的指标，即只增不减。Gauge类型代表一种样本数据可以任意变化的指标，即可增可减。例如，可以使用counter类型的指标来表示服务的请求数、错误发生的次数等。而内存使用率这种指标数据可以用Gauge表示。

* Histogram：直方图。
* Summary：摘要。

而后两种指标类型Histogram与Summary则是为了解决长尾问题。以平均响应时间为例，如果大多数API请求都维持在100ms的响应时间范围内，而个别请求的响应时间需要5s，就会导致结果与预期产生较大的偏差。Histogram 在一段时间范围内对数据进行采样（通常是请求持续时间或响应大小等），并将其计入可配置的存储桶（bucket）中，后续可通过指定区间筛选样本，也可以统计样本总数，最后一般将数据展示为直方图。而Summary与 Histogram类型类似，用于表示一段时间内的数据采样结果（通常是请求持续时间或响应大小等），但它直接存储了分位数（通过客户端计算，然后展示出来），而不是通过区间来计算。

实际上，在基准测试章节中，我们已经使用到了Histogram与Summary两种指标，如下：

<div class="image-container">
    <img src="./docs/im/images/236.png" alt="图片30-7" title="图片30-7" >
    <span class="image-title">图 30-7 </span>
</div>

了解了基本套路逻辑之后，接下来就是项目中代码实战。

## 项目实战

### 指标设计

首先，我们以网关为示例，通过集成Prometheus，实现如下业务指标的监控。

<div class="image-container">
    <img src="./docs/im/images/237.png" alt="图片30-8" title="图片30-8" >
    <span class="image-title">图 30-8 </span>
</div>

* channel_total: 在线channel数量，即当前登录到此网关的人数。
* message_in_total：网关接收消息总数。
* message_in_flow_bytes: 网关接收消息字节数。
* message_out_flow_bytes: 网关下发的消息字节数。
* no_server_found_error_total：在lookup方法中，查找zone分区中服务失败的次数。

### 集成Library

> 集成prometheus非常简单，只需要两步即可实现对外提供默认的Metrics接口。

首先，引入prometheus提供的client_golang相关包。如下：

```shell
go get github.com/prometheus/client_golang/prometheus
go get github.com/prometheus/client_golang/prometheus/promauto
go get github.com/prometheus/client_golang/prometheus/promhttp
```

其次，通过提供一个 /metrics 站点，对外暴露prometheus格式的metrics，这里就需要使用到promhttp库。在kim中，之前就已经在容器container包中提供了Http监控服务，此时只需要添加一行代码即可。如下：

```go
//container/container.go

func EnableMonitor(listen string) {
	c.monitor.Do(func() {
		go func() {
			http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
				_, _ = w.Write([]byte("ok"))
			})
			// add prometheus metrics
			http.Handle("/metrics", promhttp.Handler())    <-- new
			_ = http.ListenAndServe(listen, nil)
		}()
	})
}
```

启动网关，访问`http://localhost:8001/metrics` ，可以看到prometheus golang library给我们提供的默认的metrics。

<div class="image-container">
    <img src="./docs/im/images/238.png" alt="图片30-9" title="图片30-9" >
    <span class="image-title">图 30-9 </span>
</div>

上图只是这个接口提供的一部分metrics，从上图中我们可以看到如gc_duration、goroutines、memstats_alloc等非常重要的指标，基本上包括了golang运行时相关的所有重要metrics。通过对这些metrics的监控及创建报警规则，可以帮忙我们及时发现线上服务异常。

### 自定义metrics

接下来，就进入coding环节，实现自定义metrics。以channel_total指标为例，由于channel的管理在DefaultServer中，因此我们在它的同包kim下创建一个metrics.go文件，并定义一个变量。代码如下：

```go
//metrics.go

package kim

import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

var channelTotalGauge = promauto.NewGaugeVec(prometheus.GaugeOpts{
	Namespace: "kim",
	Name:      "channel_total",
	Help:      "The total number of channel logined",
}, []string{"serviceId", "serviceName"})
```

需要说明的一点就是，这里使用到了promauto.NewGaugeVec，它会自动把当前Collector注册到prometheus的Registry中。在早期版本中，如下两步是由业务层自己实现的。

```go
func (f Factory) NewGaugeVec(opts prometheus.GaugeOpts, labelNames []string) *prometheus.GaugeVec {
	g := prometheus.NewGaugeVec(opts, labelNames)     <---new
	if f.r != nil {
		f.r.MustRegister(g)                       <---register
	}
	return g
}
```

回到channelTotalGauge的定义中来，这里定义时设置两个标签serviceId、serviceName，之后在使用channelTotalGauge时就需要把这两个label的值传过来，这样在grafana中配置数据面板时就可以根据serviceName做过滤。

其中，Gauge提供如下几个方法：

```go
type Gauge interface {
	Metric
	Collector

	// Set sets the Gauge to an arbitrary value.
	Set(float64)
	// Inc increments the Gauge by 1. Use Add to increment it by arbitrary
	// values.
	Inc()    <-- 加1
	// Dec decrements the Gauge by 1. Use Sub to decrement it by arbitrary
	// values.
	Dec()    <-- 减1
	// Add adds the given value to the Gauge. (The value can be negative,
	// resulting in a decrease of the Gauge.)
	Add(float64)
	// Sub subtracts the given value from the Gauge. (The value can be
	// negative, resulting in an increase of the Gauge.)
	Sub(float64)

	// SetToCurrentTime sets the Gauge to the current Unix time in seconds.
	SetToCurrentTime()
}
```

接下来就需要在channel创建及销毁的代码块中，使用channelTotalGauge这个GaugeVec。代码如下：

```go
func (s *DefaultServer) connHandler(rawconn net.Conn, gpool *ants.Pool) {
	... 省略
        s.Add(channel)
        
        gaugeWithLabel := channelTotalGauge.WithLabelValues(s.ServiceID(), s.ServiceName())
	gaugeWithLabel.Inc()            <-- 增加一个channel
	defer gaugeWithLabel.Dec()      <-- 如果channel.Close()，再减1.

	logger.Infof("accept channel - ID: %s RemoteAddr: %s", channel.ID(), channel.RemoteAddr())
	err = channel.Readloop(s.MessageListener)
	if err != nil {
		logger.Info(err)
	}
	s.Remove(channel.ID())
	_ = s.Disconnect(channel.ID())
	channel.Close()
}
```

可以看到，使用非常简单，不过在使用中要注意Inc()和Dec()一定要组合使用，如果因为代码异常之类的没有执行Dec() 就会导致值泄漏，永远都不会变为0，而使用Set(float64)就不会有这个问题。

### 效果演示

首先，我们把整个kim集群运行起来。然后使用kimbench执行登录测试。

> `./kimbench benchmark login -c 100 -k 30s`

为了使方便从/metrics中查询这个指标，这里使用-k 30s参数表示登录之后保持30s，然后再断开连接。最终得到下图：

<div class="image-container">
    <img src="./docs/im/images/239.png" alt="图片30-10" title="图片30-10" >
    <span class="image-title">图 30-10 </span>
</div>

在测试程序退出之后，再次访问/metrics接口，得到下图：

<div class="image-container">
    <img src="./docs/im/images/240.png" alt="图片30-11" title="图片30-11" >
    <span class="image-title">图 30-11 </span>
</div>

> 接下来，我们把这个/metrics集成到prometheus中。

首先，使用docker在本地启动一个server。

```cmd
docker run --name prometheus -d -p 9090:9090 \
-v ~/data/prometheus:/etc/prometheus \
quay.io/prometheus/prometheus
```

在启动时，服务会从外挂的 `~/data/prometheus/prometheus.yml` 文件中加载配置。文件的内容已经提前写好了。内容如下：

```yaml
scrape_configs:
  - job_name: 'gateway'
    scrape_interval: 5s
    static_configs:
      - targets: ['192.168.3.201:8001']    <---gateway暴露的监控站点
```

> 注意scrape_interval参数，prometheus会根据这个间隔时间去targets站点上拉取数据。因此在测试时，Gauge指标保持时间最好超过这个间隔时间，否则有可能再次拉取时刚好错过了有数据的时间段。

等待prometheus启动完成之后。打开`http://127.0.0.1:9090/targets`，可以看到已经成功的界面。如图：

<div class="image-container">
    <img src="./docs/im/images/241.png" alt="图片30-12" title="图片30-12" >
    <span class="image-title">图 30-12 </span>
</div>

点击Graph菜单，就可以打开PromQL界面。在query框中输入kim_channel_total，执行之后就可以查询到这个metric的数据。如下图：

<div class="image-container">
    <img src="./docs/im/images/242.png" alt="图片30-13" title="图片30-13" >
    <span class="image-title">图 30-13 </span>
</div>

此时没有数据，因此需要一点测试数据，因此再次使用到kimbench工具。经过一翻登录测试之后，得到如下图：

<div class="image-container">
    <img src="./docs/im/images/243.png" alt="图片30-14" title="图片30-14" >
    <span class="image-title">图 30-14 </span>
</div>

同时，也可以看到内存分配情况。如下：

<div class="image-container">
    <img src="./docs/im/images/244.png" alt="图片30-15" title="图片30-15" >
    <span class="image-title">图 30-15 </span>
</div>

同一时间段协程数量图。如下：

<div class="image-container">
    <img src="./docs/im/images/245.png" alt="图片30-16" title="图片30-16" >
    <span class="image-title">图 30-16 </span>
</div>

从图中可以看到goroutine的数量与channel的关系，因为一个channel占用2个goroutine，所以当在线用户达到1k时，goroutine数量为2k多一点。

不过，prometheus提供的界面比较简单，而且panel也无法保存，因此在实际场景中，它会被当作grafana的一个数据源来配置各种类型的监控面板，不过这方面的教程非常多，不是本章的重点，因此就不在这里贴文档了。

## 最后总结

本章节首先重点介绍了云原生下可观测性的重要性及监控、日志和追踪相关的技术方案，并结合prometheus实战，重点介绍了监控相关的知识点；当然本章只介绍了后端的相关监控技术，在实际场景中，还要结合用户SDK端的上报数据，打造一个全方面的监控报警系统。

最后，在如今开发运维不分家的DevOps时代，这句话送给读者：

> 无监控，不运维！

本章完！
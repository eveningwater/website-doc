# 进阶：性能优化之存储

<div class="image-container">
    <img src="./docs/im/images/205.png" alt="图片27-1" title="图片27-1" >
    <span class="image-title">图 27-1 </span>
</div>

在即时通信系统中，主要有两方面考验系统的整体性能：

* 并发数
* 消息吞吐量

在前面的两个章节中，通过通信层的一些优化，网关的内存占用与cpu消耗有了极大的提升，基本上一台4核8G内存的网关节点可以轻松达到10w+的并发。也就是说通过网关的水平扩展，或提高网关节点的硬件资源配置，达到百万或者千万的并发很容易。此时，整个系统的性能压力就落到具体的业务逻辑层，也就是登录、单聊、群聊等逻辑。

而在其中，登录的性能主要与会话的写性能相关，而单聊和群聊的性能瓶颈取决于寻址速度与消息存储耗时，不过采用mysql与消息扩展写的方案，消息存储吞吐量优化空间有限，因此本章就不涉及消息写相关的优化。

> 本章节我们主要介绍寻址优化相关的知识点。

## 寻址优化

优化寻址的吞吐量就是提高redis读吞吐量，通常一台Redis实例的读QPS可以达到5w左右，也就是说，对于一个100人的群来说，如果忽略消息存储所占用的时间，消息转发的吞吐量极限值也就是500了。

> 那么在不提高硬件资源的前期下，如何提高寻址吞吐量呢?

### 主从复制

Redis具有主从复制的方案，也就是可以配置一到多个从节点，主节点的数据会同步给从节点。那么，我们是否可以利用这个方案，通过随机或者轮询主从节点，提高读性能呢？

<div class="image-container">
    <img src="./docs/im/images/206.png" alt="图片27-2" title="图片27-2" >
    <span class="image-title">图 27-2 </span>
</div>

> 要回答这个问题，就要理解redis的同步原理！

简单来说redis的同步机制是异步模式，即数据写入主节点时不会等待数据的同步完成。也就是说，用户A登录时，会话写入主节点返回之后，此时用户B给用户A发送一条消息，如果寻址时读取的是slave节点，就可能会因为master节点的数据还未同步到slave节点，导致寻址返回空，最终的结果就是这条消息不会推送给用户A。如下图所示：

<div class="image-container">
    <img src="./docs/im/images/207.png" alt="图片27-3" title="图片27-3" >
    <span class="image-title">图 27-3 </span>
</div>

可以看到，如果UserB发送消息时，直接访问master节点不会有问题。但是访问slave节点，就会存在不确定性，如果寻址时数据还未同步过来，就会寻址失败。

> 因此，读主从的方案不可行。

### 多写多读

如果说redis默认的主从同步会导致数据不一致。那么在业务层，可以通过多写多读的方式解决数据一致性问题。如下图所示：

<div class="image-container">
    <img src="./docs/im/images/208.png" alt="图片27-4" title="图片27-4" >
    <span class="image-title">图 27-4 </span>
</div>

在这种情况下，多个主节点中，会话的写是由Login服务完成，它是可以保证数据一致性的，但另一方面就会导致可用性降低，也就是满足CAP原理中的CP性。此时，无论读取哪一个redis节点，数据都是一致的。不过，这个方案下，登录时因为要写多次会话，吞吐量会有一点下降，但是也不会太大，毕竟都是内存操作。而且，在即时通信系统中，通常情况是消息的收发量比登录请求的量要大很多，因此牺牲一点登录性能，但是可以双倍提高寻址性能，也是一个不错的选择。

### 数据分片

采用双写的方案是一个最简单，成本较低的扩展方案，不过性能是有一定上限的，只适合体量不太大的场景。因此，这里介绍另一种扩展性更好的方案，也是分布式系统中最常见的一种数据扩展方案数据分片。即通过把数据按照规则划分到不同的存储节点上，达到读写水平扩展的目的。如下图中不同的key通过计算最终落到了不同的存储节点中：

<div class="image-container">
    <img src="./docs/im/images/209.png" alt="图片27-5" title="图片27-5" >
    <span class="image-title">图 27-5 </span>
</div>

只要规则f(x)不变，那么每次读与写最终都是落到相同的节点上，就不会存在数据一致性问题，而且也解决了扩展问题。当然，它也存在一定的问题，例如：

1. 群成员批量寻址时，由于数据离散在不同的节点，mget操作就会失效。
2. 数据存储可能会产生不均衡，由分片规则决定。

> 那么，如何实现数据分片呢？

这里介绍三种方案：

1. 由业务层主动维护多个节点，以及决定分片规则。
2. 采用redis集群代理Proxy方案。
3. 使用redis cluster集群方案。

每种方案都会有它的优势与缺点。其中第1和第3这两种方案的逻辑比较类似，特点就是客户端直接与redis节点通信，并且是每个客户端都要与所有的redis节点建立连接。

<div class="image-container">
    <img src="./docs/im/images/210.png" alt="图片27-6" title="图片27-6" >
    <span class="image-title">图 27-6 </span>
</div>

而代理方案中客户端是与代理层通信，代理层解析并处理客户端的命令。如下图所示：

<div class="image-container">
    <img src="./docs/im/images/211.png" alt="图片27-7" title="图片27-7" >
    <span class="image-title">图 27-7 </span>
</div>

redis代理的原理与http代理服务（如nginx）的逻辑是相同的，只不过协议格式不同，对一个redis客户端来说，代理节点就是一个正常的redis节点。

> 注意：以上都是逻辑示意图，还没有涉及到高可用方面的知识点。

如此一来，以上三种方案的分片规则的管理逻辑就落在了不同的层级。如下图：

<div class="image-container">
    <img src="./docs/im/images/212.png" alt="图片27-8" title="图片27-8" >
    <span class="image-title">图 27-8 </span>
</div>

1. manual：手动维护多个节点情况下，分片规则由业务层管理。
2. proxy: 代理方案下，redis集群的分片规则由代理服务统一管理，至于它是手动配置，还是通过平台统一配置，则是由相应的中间件决定。比如 [twemproxy](https://github.com/twitter/twemproxy) 和 [codis](https://github.com/CodisLabs/codis) 两种redis代理。
3. redis cluster: 采用redis集群方案时，分片规则在redis节点中管理，但是最终会同步给redis客户端SDK中。

> 那么，如何实现数据的分片呢？

## 分片算法

### 一致性Hash

通常数据分片其中一个算法就是一致性Hash。比如有N个节点，那么通过hash取模可以使一个key落在一个固定的节点上。如下：

`i = hash(key) % N`

也就是一个key会落在第i个节点上。其中hash算法有很多，比如常用的crc32、crc16等，如下是一个简单的代码示例：

```go
hash32 := crc32.NewIEEE()
hash32.Write([]byte(key))

N := 3                     <-- 3个节点
i := hash32.Sum32() % N    <-- 位置i
```

不过，使用一致性Hash的缺点就是扩容或缩容时，也就是N发生变化，会导致i变化，由于数据还是存储在原来的节点上，会导致大量key无法命中。例如在一个有3个节点的集群中，如果Hash(key)的结果是9，那么当前key就会落在第0个节点上，当N变化时：

* 如 N=2，则 9 % 2 = 1；节点改变。
* 如 N=1，则 9 % 1 = 0；节点未改变。
* 如 N=4，则 9 % 4 = 1；节点改变。
* 如 N=5，则 9 % 5 = 4；节点改变。
* 如 N=6，则 9 % 6 = 3；节点改变。

具体细节就不推演了，节点发生变化，如果数据不迁移到新的节点上，那么key就无法命中。对一个缓存场景的业务来说，就意味着大量的请求会落到数据库，可能会导致数据库的崩溃，进而引发雪崩效应。而对于本小册kim中的寻址来说，就意味着消息的丢失，或者是丢失方的重新登录。因此，如果采用自定义分片方案，扩容时比较麻烦，通常在用户最不活越的时间段扩容升级。

### hash slots

除了上面的一致性Hash的分片方案，还有一致性Hash环的方案可以降低扩容导致的数据变更。而在redis cluster中，则是采用另一种方案hash slots。它是在一致性Hash的基础上变化而来，其中N变为一个虚拟数值16384。公式变为：

`i = hash(key) % 16384`

也就是只要知道key与hash算法，就可以得到一个固定的i，然后再把slot划分给不同的节点，如下配置示例：

> Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where:

* Node A contains hash slots from 0 to 5500.
* Node B contains hash slots from 5501 to 11000.
* Node C contains hash slots from 11001 to 16383.

也就是i的值在0 ~ 5500时，数据存储在节点A上，5501 ~ 11000存储在B节点，11001 ~ 16383存储在C节点。那么扩容时，只要修改这个配置信息就可以了，如增加一个节点D，可以把A、B、C三个节点中的slot分一部分给到D即可，比如A节点中的4001 ~ 5500分给D，B节点中的9001 ~ 11000分给D，C节点中的15001 ~ 16383分给D。就得到如下结果：

* Node A: [0, 4000]
* Node B: [5501, 9000]
* Node C: [11001, 15000]
* Node D: [4001, 5500], [9001, 11000], [15001, 16383]

最后，使用不同的方案，寻址时改动的逻辑不同。比如采用proxy几乎不需要修改寻址逻辑，但是会增加架构及运维的复杂度。采用redis cluster方案需要对现有代码做修改，并且要解决数据分片情况下批量寻址问题，不过我们可以使用redis-go-cluster库来帮助我们执行MGET操作。示例如下：

```go
import "github.com/chasex/redis-go-cluster"

cluster, err := redis.NewCluster(
    &redis.Options{
	StartNodes: []string{"127.0.0.1:7000", "127.0.0.1:7001", "127.0.0.1:7002"},
	ConnTimeout: 50 * time.Millisecond,
	ReadTimeout: 50 * time.Millisecond,
	WriteTimeout: 50 * time.Millisecond,
	KeepAlive: 16,
	AliveTime: 60 * time.Second,
    })

reply, err := Values(cluster.Do("MGET", "key1", "key2", "key3", "key4"))
```

### storage优化

回到kim项目中，我们在storage包中，创建一个新的kim.SessionStorage实现类，通过redis-go-cluster来实现相应的方法，其中批量寻址的方法修改如下：

```go
//storage/redis_cluster_impl.go
package storage

import (
	"time"

	"github.com/chasex/redis-go-cluster"
	"github.com/klintcheng/kim"
	"github.com/klintcheng/kim/wire/pkt"
	"google.golang.org/protobuf/proto"
)

type RedisClusterStorage struct {
	cli redis.Cluster
}

func NewRedisClusterStorage(cli redis.Cluster) kim.SessionStorage {
	return &RedisClusterStorage{
		cli: cli,
	}
}

func (r *RedisClusterStorage) GetLocations(accounts ...string) ([]*kim.Location, error) {
	keys := KeyLocations(accounts...)
	list, err := redis.Values(r.cli.Do("MGET", keys))   <--- 修改调用方式
	if err != nil {
		return nil, err
	}
	var result = make([]*kim.Location, 0)
	for _, l := range list {
		if l == nil {
			continue
		}
		var loc kim.Location
		_ = loc.Unmarshal([]byte(l.(string)))
		result = append(result, &loc)
	}
	if len(result) == 0 {
		return nil, kim.ErrSessionNil
	}
	return result, nil
}
```

实际上，它在底层就是通过hashslot分组处理上层Do中的命令，我们通过它的源码分析下它的实现原理，代码如下:

```go
//redis-go-cluster/cluster.go
package redis

const (
    kClusterSlots	= 16384
)

type redisCluster struct {
    slots	[]*redisNode     <--- 16384个，根据cluster节点信息实例化得到。
    nodes	map[string]*redisNode

    connTimeout time.Duration
    readTimeout time.Duration
    writeTimeout time.Duration

    keepAlive	int
    aliveTime	time.Duration

    updateTime	time.Time
    updateList	chan updateMesg

    rwLock	sync.RWMutex
}

func (cluster *redisCluster) multiGet(cmd string, args ...interface{}) (interface{}, error) {
    tasks := make([]*multiTask, 0)           <-- 1. 任务分组
    index := make([]*multiTask, len(args))

    cluster.rwLock.RLock()
    for i := 0; i < len(args); i++ {
	key, err := toBytes(args[i])
	if err != nil {
	    cluster.rwLock.RUnlock()
	    return nil, fmt.Errorf("invalid key %v", args[i])
	}

	slot := hashSlot(key)            <--- 2. 计算hashslot

	var j int
	for j = 0; j < len(tasks); j++ {
	    if tasks[j].slot == slot {     <--- 更新task
		tasks[j].args = append(tasks[j].args, args[i])	    // key
		index[i] = tasks[j]

		break
	    }
	}

	if j == len(tasks) {
	    node := cluster.slots[slot]     <--- 3. 定位node
	    if node == nil {
		cluster.rwLock.RUnlock()
		return nil, fmt.Errorf("no node serve slot %d for key %s", slot, key)
	    }

	    task := &multiTask{           <--- 4. 生成task
		node: node,
		slot: slot,
		cmd: cmd,
		args: []interface{}{args[i]},
		done: make(chan int),
	    }
	    tasks = append(tasks, task)
	    index[i] = tasks[j]
	}
    }
    cluster.rwLock.RUnlock()

    for i := range tasks {
	go handleGetTask(tasks[i])   <---- 5. 执行task
    }

    for i := range tasks {
	<-tasks[i].done
    }

    reply := make([]interface{}, len(args))
    for i := range reply {
	if index[i].err != nil {
	    return nil, index[i].err
	}

	if len(index[i].replies) < 0 {
	    panic("unreachable")
	}

	reply[i] = index[i].replies[0]
	index[i].replies = index[i].replies[1:]
    }

    return reply, nil
}
```

以上逻辑主要为5步：

1. 任务分组：根据node不同对key做分组，相同node的key会落到同一个组。
2. 计算hashslot：通过crc16计算hashslot。
3. 定位node：通过slot定位出node，注意slots切片的长度为kClusterSlots。slots的值是动态更新的，详见updateClustrInfo方法。
4. 创建task：为每个需要执行的node分配一个任务，其中args落在当前节点中的keys。
5. 执行task：异步执行task。

最后就是等待所有task返回结果，生成reply返回给上层。

## 最后总结

本章主要介绍了分布式存储相关的知识点，如：

1. 主从同步
2. 双写双读
3. 数据分片

同时，在实战项目中还利用到了Mysql分区表的功能，解决千万级以上数据情况下离线消息同步的问题，感兴趣的读者可以直接查看源码，其中创建分区表的sql文件为[message_index.sql](https://github.com/klintcheng/kim/blob/feature/parttion_hash/message_index.sql)。

本章完！
# 进阶：性能优化之缓冲

<div class="image-container">
    <img src="./docs/im/images/194.png" alt="图片26-1" title="图片26-1" >
    <span class="image-title">图 26-1 </span>
</div>

## 空间换时间

缓冲与缓存虽然有本质区别，但是它们都是利用空间换时间的手段。

### 缓存简介

缓存是利用读写性能更高的介质来达到提高系统整体性能的目的。比如CPU的一级、二级缓存，高端CPU还具有三级缓存，它的读写速度要远高于内存；而内存相对于硬盘，读写性能又要高一个量级，也是被当作缓存使用最多的存储介质。如下是各存储介质的访问性能：

| 存储介质 | 容量   | 访问速度     |
| -------- | ------ | ------------ |
| 寄存器   | 几百B  | 1 纳秒       |
| 一级缓存 | 几百KB | 4 纳秒       |
| 二级缓存 | 几MB   | 40 ~ 60 纳秒 |
| 内存     | GB     | 100 纳秒     |
| SSD      | GB     | 16 微秒      |
| 硬盘     | TB     | 2 毫秒       |

可以从这里看到不同年代硬件性能的对比【[colin-scott](https://colin-scott.github.io/personal_website/research/interactive_latency.html)】，如下：

<div class="image-container">
    <img src="./docs/im/images/195.png" alt="图片26-2" title="图片26-2" >
    <span class="image-title">图 26-2 </span>
</div>

比如后端经常使用的Redis，就是一个主流的基于内存的缓存中间件。大多数读者对缓存都比较了解，在使用缓存的过程中，通常还需要考虑如下问题：

1. 命中率
2. 数据一致性
3. 剔除策略

总的来说，缓存相对来说要复杂很多，而缓冲相对来说就简单很多，只要搞明白了它的原理，自己就可以实现一个缓冲库。

> 那么，缓冲为什么可以提高性能呢 ？

### 缓冲简介

缓冲是利用一块内存空间当作一个蓄水池，通过合并多次IO读、写操作，达到提高性能的目的。如下图所示：

<div class="image-container">
    <img src="./docs/im/images/196.png" alt="图片26-3" title="图片26-3" >
    <span class="image-title">图 26-3 </span>
</div>

> 那么，为什么减少读写IO可以提高性能呢？

在操作系统中，用户态与内核态是两个不同的世界，用户态程序是无法把数据直接写入（或读取）内核态的内存空间，比如如操作系统创建的TCP缓冲区；而是通过操作系统提供的trap指令完成。比如在向一个网络socket写数据时，当前执行线程由于执行trap指令会主动让出CPU（陷入内核态），由操作系统执行相关系统调用（数据copy），完成之后再通过调度逻辑把这个线程恢复执行（得到CPU）。这个过程相对来说比较耗时，因此，减少IO操作也就在一定程序上提高了程序性能。

> 在golang的runtime中，一个核心线程M在陷入之前，挂在它上面的处理器P会被移走，不会阻塞之后的G(goroutine)执行。

## 如何优化

### 性能分析

首先，编写一个压力测试脚本，发送10w条消息给服务端，服务端在收到之后会回复一个ok，为了达到测试的目的，消息的大小不能过大。代码如下：

```go
// examples/benchmark/server_test.go

func Test_Message(t *testing.T) {
	const count = 1000 * 100               <---发送10w条消息
	cli := websocket.NewClient(fmt.Sprintf("test_%v", 1), "client", websocket.ClientOptions{
		Heartbeat: kim.DefaultHeartbeat,
	})
	// set dialer
	cli.SetDialer(&mock.WebsocketDialer{})

	// step2: 建立连接
	err := cli.Connect(wsurl)
	if err != nil {
		logger.Error(err)
	}
	msg := []byte(strings.Repeat("hello", 10))   <-- 消息内容50个字符，不可超过缓冲区大小。
	t0 := time.Now()
	go func() {
		for i := 0; i < count; i++ {
			_ = cli.Send(msg)
		}
	}()
	recv := 0
	for {
		frame, err := cli.Read()
		if err != nil {
			logger.Info("time", time.Now().UnixNano(), err)
			break
		}
		if frame.GetOpCode() != kim.OpBinary {
			continue
		}
		recv++
		if recv == count { // 接收完消息
			break
		}
	}

	t.Logf("message %d cost %v", count, time.Since(t0))
}
```

测试过程与上一章节相同，这里就不再介绍了。通过cpu分析，得到如下结果：

<div class="image-container">
    <img src="./docs/im/images/197.png" alt="图片26-4" title="图片26-4" >
    <span class="image-title">图 26-4 </span>
</div>

耗时操作主要为读与写两步产生：

1.writeloop 这个goroutine中的异步消息写，耗时1.21s。

<div class="image-container">
    <img src="./docs/im/images/198.png" alt="图片26-5" title="图片26-5" >
    <span class="image-title">图 26-5 </span>
</div>

2.Readloop 这个goroutine中的消息读取，耗时1.21s。

<div class="image-container">
    <img src="./docs/im/images/199.png" alt="图片26-6" title="图片26-6" >
    <span class="image-title">图 26-6 </span>
</div>

其中，读写消息又分为两步：

1. 读、写Header。
2. 读、写Payload。

这就导致在一次消息的读或写的过程中，至少有两次IO操作。它们最终都会执行syscall.Write和syscall.Read。

> 为了减少IO操作，可以通过创建一个缓冲区，把多次读或写的IO操作变为一次IO操作；

这里就要使用到bufio这个库了，它提供了读、写两个实现类，如下：

* bufio.Writer：写缓冲
* bufio.Reader：读缓冲

### bufio.Writer原理

下图是一个写缓冲的示意图：

<div class="image-container">
    <img src="./docs/im/images/200.png" alt="图片26-7" title="图片26-7" >
    <span class="image-title">图 26-7 </span>
</div>

它相对读缓冲更好理解，如上图所示，在未使用缓冲之前，写2条消息(Frame)就需要4次IO操作；而使用缓冲之后，只需要一次IO操作即可。不过，这里需要思考一个问题：

> 如果写入的数据超过了写缓冲的可用空间怎么办？

这个问题，可以从源码中得到答案。如下是bufio中Writer实现代码：

```go
package bufio

// Writer implements buffering for an io.Writer object.
// If an error occurs writing to a Writer, no more data will be
// accepted and all subsequent writes, and Flush, will return the error.
// After all data has been written, the client should call the
// Flush method to guarantee all data has been forwarded to
// the underlying io.Writer.
type Writer struct {
	err error
	buf []byte          <--- byte缓冲池
	n   int             <--- 已填充偏移量
	wr  io.Writer       <--- 底层Writer，在本章节的示例中就是一个连接net.Conn。
}

func NewWriterSize(w io.Writer, size int) *Writer {
	// Is it already a Writer?
	b, ok := w.(*Writer)
	if ok && len(b.buf) >= size {
		return b
	}
	if size <= 0 {
		size = defaultBufSize
	}
	return &Writer{
		buf: make([]byte, size),
		wr:  w,
	}
}

// Write writes the contents of p into the buffer.
// It returns the number of bytes written.
// If nn < len(p), it also returns an error explaining
// why the write is short.
func (b *Writer) Write(p []byte) (nn int, err error) {
	for len(p) > b.Available() && b.err == nil {   <--- 超过可用空间
		var n int
		if b.Buffered() == 0 {
			// Large write, empty buffer.
			// Write directly from p to avoid copy.
			n, b.err = b.wr.Write(p)       <--- 情况一
		} else {
			n = copy(b.buf[b.n:], p)       <--- 情况二
			b.n += n
			b.Flush()
		}
		nn += n
		p = p[n:]
	}
	if b.err != nil {
		return nn, b.err
	}
	n := copy(b.buf[b.n:], p)                     <--- p copy到缓冲中
	b.n += n
	nn += n
	return nn, nil
}

// Available returns how many bytes are unused in the buffer.
func (b *Writer) Available() int { return len(b.buf) - b.n }

// Buffered returns the number of bytes that have been written into the current buffer.
func (b *Writer) Buffered() int { return b.n }
```

如上所示，当写入的数据p的长度大于缓冲区的可用空间长度时，就会进入如下两种情况：

* 情况一：当前缓冲区为空，说明此数据p的长度要大于缓冲池的长度len(b.buf)；这种情况下，直接把消息写入底层Writer（可以认为是net.Conn）。
* 情况二：当前缓冲区已经有数据（b.Buffered() > 0），就用p的前面一部分数据把b.buf填满，然后主动执行一次b.Flush() 操作，把缓冲buf中的数据写入Writer，相应的buf也会被清空。如果数据p中另一部分数据还是超过可用空间，那么在for的第二次执行时，由于buf已经为空，就会进入情况一。

回到Kim中的通信层逻辑中channel.writeloop()，我们来分析下使用写缓冲优化的边界问题：

* 在最坏的情况下，执行Flush时，缓冲中只有一帧数据，也就是只是减少了一次IO操作，但是增加了一次内存Copy。
* 在最理想的情况下，执行Flush时，缓冲区被N个数据帧填满，也就减少了 N ∗ 2 − 1 次IO操作。

### bufio.Reader原理

读缓冲的原理与写缓冲类似。不过，读缓冲依赖内核空间中的tcp缓冲区中是否有足够的数据，在一种较理想的情况下，可以一次IO读取足够的数据把缓冲区填满，这样上层业务读取数据时，就可以直接从缓冲区读取，直到把缓冲区中的数据读完为止。如下是读缓冲的示意图：

<div class="image-container">
    <img src="./docs/im/images/201.png" alt="图片26-8" title="图片26-8" >
    <span class="image-title">图 26-8 </span>
</div>

在上图中，刚好有两帧数据P1和P2分别被两次读取，达到了一个较理想的情况。但是TCP是流式传输，是不保证应用层协议数据完整性的，也就是说另一种情况下，P2只有部分数据在TCP读缓冲中，被第一次IO读取到了应用层缓冲区；这种情况下，读取P1之后，第二次读取P2时就会产生数据缺失，此时就需要业务层再次读取，如果此时TCP读缓冲区没有数据，那么当前线程G就会休眠，直到有数据到来才会被处理器P唤醒读取数据。这也是为什么在上层，读取数据时使用io.ReadFull(r, buf)的原因。

接下来，我们通过分析源码来了解更多细节逻辑。如下是bufio.Reader主要实现代码：

```go
package bufio

// Reader implements buffering for an io.Reader object.
type Reader struct {
	buf          []byte
	rd           io.Reader // reader provided by the client
	r, w         int       // buf read and write positions
	err          error
	lastByte     int // last byte read for UnreadByte; -1 means invalid
	lastRuneSize int // size of last rune read for UnreadRune; -1 means invalid
}

// Read reads data into p.
// It returns the number of bytes read into p.
// The bytes are taken from at most one Read on the underlying Reader,
// hence n may be less than len(p).
// To read exactly len(p) bytes, use io.ReadFull(b, p).
// At EOF, the count will be zero and err will be io.EOF.
func (b *Reader) Read(p []byte) (n int, err error) {
	n = len(p)
	if n == 0 {
		if b.Buffered() > 0 {
			return 0, nil
		}
		return 0, b.readErr()
	}
	if b.r == b.w {                                  <-- 缓冲区为空
		if b.err != nil {
			return 0, b.readErr()
		}
		if len(p) >= len(b.buf) {                <-- p的长度超过缓冲区大小
			// Large read, empty buffer.
			// Read directly into p to avoid copy.
			n, b.err = b.rd.Read(p)          <-- 直接读取，避免一次copy。
			if n < 0 {
				panic(errNegativeRead)
			}
			if n > 0 {
				b.lastByte = int(p[n-1])
				b.lastRuneSize = -1
			}
			return n, b.readErr()
		}
		// One read.
		// Do not use b.fill, which will loop.
		b.r = 0
		b.w = 0
		n, b.err = b.rd.Read(b.buf)              <-- 读取buf
		if n < 0 {
			panic(errNegativeRead)
		}
		if n == 0 {
			return 0, b.readErr()
		}
		b.w += n                                 <-- 写位置向后移动n位
	}

	// copy as much as we can
	n = copy(p, b.buf[b.r:b.w])                      <-- 从缓冲buf中copy数据到p中
	b.r += n                                         <-- 读位置向后移动n位
	b.lastByte = int(b.buf[b.r-1])
	b.lastRuneSize = -1
	return n, nil
}
```

与写缓冲不同的是，Reader通过buf和r, w维护了一个环形缓冲区，当r读索引与w写索引相同时，buf为空。只有在buf为空时会产生IO读的动作，如果p的长度超过缓冲区大小，就直接从底层reader读取，避免一次copy。如果buf不为空，会优先从缓冲读取数据。

## 优化实战

### 缓冲区优化

搞明白了原理，那么优化的逻辑就很简单了，只要在读写net.Conn的地方，修改为操作缓冲对象即可，以websocket中的Conn为例。修改如下：

```go
type WsConn struct {
	net.Conn
	rd *bufio.Reader
	wr *bufio.Writer
}

func NewConn(conn net.Conn) kim.Conn {
	return &WsConn{
		Conn: conn,
		rd:   bufio.NewReaderSize(conn, 4096),             <--- 创建一个大小为4KB的读缓冲
		wr:   bufio.NewWriterSize(conn, 1024),             <--- 创建一个大小为1KB的写缓冲
	}
}

func (c *WsConn) ReadFrame() (kim.Frame, error) {
	f, err := ws.ReadFrame(c.rd)                              <--- 替换
	if err != nil {
		return nil, err
	}
	return &Frame{raw: f}, nil
}

func (c *WsConn) WriteFrame(code kim.OpCode, payload []byte) error {
	f := ws.NewFrame(ws.OpCode(code), true, payload)
	return ws.WriteFrame(c.wr, f)                            <--- 替换
}

func (c *WsConn) Flush() error {
	return c.wr.Flush()                                      <--- 新增
}
```

可以看到，对原有的通信层改动不大，只是把ReadFrame和WriteFrame方法中的net.Conn替换成对应的wr和rd。不过，只调用WriteFrame，往缓冲区写数据是不够的，通过前面的分析，可以得知写缓冲在未填充满的情况下不会Flush数据到底层TCP缓冲中。因此，在业务层必须主动触发这个逻辑，否则数据的发送就产生了不可确定性。

而Flush() 方法是在Channel的writeloop方法中被调用的。如下：

```go
//channel.go
func (ch *ChannelImpl) writeloop() error {
	for {
		select {
		case payload := <-ch.writechan:
			err := ch.WriteFrame(OpBinary, payload)
			if err != nil {
				return err
			}
			chanlen := len(ch.writechan)
			for i := 0; i < chanlen; i++ {
				payload = <-ch.writechan
				err := ch.WriteFrame(OpBinary, payload)
				if err != nil {
					return err
				}
			}
			err = ch.Flush()                           <---主动Flush数据
			if err != nil {
				return err
			}
		case <-ch.closed.Done():
			return nil
		}
	}
}
```

虽然通过缓冲区提高了IO性能，但是每创建一个连接，就要分配5KB的缓冲区空间，断开之后这两个缓冲区又要被GC回收，这对一个 10W+以上并发连接的网关来说，无疑会极大的增加GC的负担，导致CPU使用率增高

### 缓冲区复用

为了降低GC回收压力，就需要对缓冲区进行复用，这利用到了sync.Pool对象池技术，这个技术在逻辑服务的Context对象复用时使用过。不过在gobwas这个库中，已经有现成的复用缓冲池库pbufio可以使用，也就无需造个轮子了。

```go
package pbufio

import (
	"bufio"
	"io"

	"github.com/gobwas/pool"
)

var (
	DefaultWriterPool = NewWriterPool(256, 65536)
	DefaultReaderPool = NewReaderPool(256, 65536)
)

// GetWriter returns bufio.Writer whose buffer has at least size bytes.
// Note that size could be ceiled to the next power of two.
// GetWriter is a wrapper around DefaultWriterPool.Get().
func GetWriter(w io.Writer, size int) *bufio.Writer { return DefaultWriterPool.Get(w, size) }

// PutWriter takes bufio.Writer for future reuse.
// It does not reuse bufio.Writer which underlying buffer size is not power of
// PutWriter is a wrapper around DefaultWriterPool.Put().
func PutWriter(bw *bufio.Writer) { DefaultWriterPool.Put(bw) }

// GetReader returns bufio.Reader whose buffer has at least size bytes. It returns
// its capacity for further pass to Put().
// Note that size could be ceiled to the next power of two.
// GetReader is a wrapper around DefaultReaderPool.Get().
func GetReader(w io.Reader, size int) *bufio.Reader { return DefaultReaderPool.Get(w, size) }

// PutReader takes bufio.Reader and its size for future reuse.
// It does not reuse bufio.Reader if size is not power of two or is out of pool
// min/max range.
// PutReader is a wrapper around DefaultReaderPool.Put().
func PutReader(bw *bufio.Reader) { DefaultReaderPool.Put(bw) }
```

它创建了两个默认的DefaultWriterPool和DefaultReaderPool，最终底层的逻辑如下：

```go
// Custom creates new Pool with given options.
func Custom(opts ...Option) *Pool {
	p := &Pool{
		pool: make(map[int]*sync.Pool),       <-- 也是使用到了sync.Pool
		size: pmath.Identity,
	}

	c := (*poolConfig)(p)
	for _, opt := range opts {
		opt(c)
	}

	return p
}


// Get pulls object whose generic size is at least of given size.
// It also returns a real size of x for further pass to Put() even if x is nil.
// Note that size could be ceiled to the next power of two.
func (p *Pool) Get(size int) (interface{}, int) {
	n := p.size(size)
	if pool := p.pool[n]; pool != nil {
		return pool.Get(), n
	}
	return nil, size
}
```

之所以不直接使用默认的sync.Pool，是因为缓冲区通常有大小的配置。pbufio做了一些包装，可以很灵活的申请不同大小的缓冲区。

使用pbufio非常简单，只要考虑好回收逻辑即可，否则错误的使用或者一个缓冲区被多个线程并发使用就会产生数据混乱。优化逻辑如下：

1.DefaultServer升级

```go
// default_server.go

func (s *DefaultServer) Start() error {
        ...省略
        
	for {
		rawconn, err := lst.Accept()
		if err != nil {
			rawconn.Close()
			log.Warn(err)
			continue
		}
		run := func(rawconn net.Conn) {
			if atomic.LoadInt32(&s.quit) == 1 {
				return
			}
			rd := pbufio.GetReader(rawconn, ws.DefaultServerReadBufferSize)
			wr := pbufio.GetWriter(rawconn, ws.DefaultServerWriteBufferSize)
			defer func() {                     <--- 回收缓冲池
				pbufio.PutReader(rd)
				pbufio.PutWriter(wr)
			}()
			conn, err := s.Upgrade(rawconn, rd, wr)
			if err != nil {
				log.Info(err)
				conn.Close()
				return
			}
                        
                        ... 省略
                        
                        err = channel.Readloop(s.MessageListener)
			if err != nil {
				log.Info(err)
			}
			s.Remove(channel.ID())
			_ = s.Disconnect(channel.ID())
			channel.Close()
                }
                go run(rawconn)

		if atomic.LoadInt32(&s.quit) == 1 {
			break
		}
	}
	log.Info("quit")
	return nil
}
```

> 注意：GetReader之后最好是利用defer调用PutReader，在方法结束之后回收缓冲区。

其中，`s.Upgrade(rawconn, rd, wr)` 最终会调用不同协议Upgrader中的Upgrade方法，以websocket为例，代码如下。

```go
//websocket/server.go

func (u *Upgrader) Upgrade(rawconn net.Conn, rd *bufio.Reader, wr *bufio.Writer) (kim.Conn, error) {
	_, err := ws.Upgrade(rawconn)
	if err != nil {
		return nil, err
	}
	conn := NewConnWithRW(rawconn, rd, wr)    <-- 在这里替换掉默认的 NewConn
	return conn, nil
}
```

2.优化websocket中的Conn实现结构体，如下：

```go
//websocket/connection.go

type WsConn struct {
	net.Conn
	rd *bufio.Reader
	wr *bufio.Writer
}

func NewConnWithRW(conn net.Conn, rd *bufio.Reader, wr *bufio.Writer) *WsConn {   <-- 创建一个新的构造器，使用外部的缓冲区
	return &WsConn{
		Conn: conn,
		rd:   rd,
		wr:   wr,
	}
}
```

至此，缓冲区的优化就结束了。接下来，再次测试下性能。

### 优化效果

再次执行上面的消息发送测试，得到如下结果：

<div class="image-container">
    <img src="./docs/im/images/202.png" alt="图片26-9" title="图片26-9" >
    <span class="image-title">图 26-9 </span>
</div>

* 其中，ReadFrame的时间缩短为0.77s，优化之前是1.21s。

<div class="image-container">
    <img src="./docs/im/images/203.png" alt="图片26-10" title="图片26-10" >
    <span class="image-title">图 26-10 </span>
</div>

* 其中，WriteFrame的时间缩短为0.63s，优化之前是1.21s。

<div class="image-container">
    <img src="./docs/im/images/204.png" alt="图片26-11" title="图片26-11" >
    <span class="image-title">图 26-11 </span>
</div>

> 在读与写的过程中，耗时的地方已经变为bufio相关方法了。

在优化逻辑之前，我们再次使用kimbench测试下系统整体优化效果。

* login: 200次请求、并发100

优化前：

```cmd
[root@hecs-x-large-2-linux-20210831100334 ~]# ./kimbench benchmark login -c 200 -t 100

Summary:
  Total:	0.0684 secs
  Slowest:	0.0494 secs
  Fastest:	0.0181 secs
  Average:	0.0303 secs      <--- 对比
  Requests/sec:	2910.3919        <--- 对比

Response time histogram:
  0.018 [1]	|
  0.026 [105]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.034 [19]	|■■■■■■■
  0.042 [32]	|■■■■■■■■■■■■
  0.049 [43]	|■■■■■■■■■■■■■■■■

Latency distribution:
  10% in 0.0187 secs
  50% in 0.0254 secs
  75% in 0.0396 secs
  90% in 0.0479 secs
  99% in 0.0490 secs

Status code distribution:
  [0]	200 responses
```

优化后：

```cmd
[root@hecs-x-large-2-linux-20210831100334 ~]# ./kimbench benchmark login -c 200 -t 100

Summary:
  Total:	0.0539 secs
  Slowest:	0.0295 secs
  Fastest:	0.0123 secs
  Average:	0.0240 secs      <--- 对比
  Requests/sec:	3694.8552        <--- 对比

Response time histogram:
  0.012 [1]	|
  0.017 [30]	|■■■■■■■■■■■■
  0.021 [0]	|
  0.025 [69]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.030 [100]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■

Latency distribution:
  10% in 0.0132 secs
  50% in 0.0270 secs
  75% in 0.0282 secs
  90% in 0.0284 secs
  99% in 0.0291 secs

Status code distribution:
  [0]	200 responses
```

* usertalk: 300次请求、并发100

优化前：

```cmd
[root@hecs-x-large-2-linux-20210831100334 ~]# ./kimbench benchmark user -c 300 -t 100

Summary:
  Total:	0.2274 secs
  Slowest:	0.1265 secs
  Fastest:	0.0294 secs
  Average:	0.0659 secs        <--- 对比
  Requests/sec:	1314.9719          <--- 对比

Response time histogram:
  0.029 [1]	|
  0.054 [97]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.078 [129]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.102 [42]	|■■■■■■■■■■■■■
  0.127 [31]	|■■■■■■■■■■

Latency distribution:
  10% in 0.0427 secs
  50% in 0.0614 secs
  75% in 0.0766 secs
  90% in 0.1023 secs
  99% in 0.1238 secs

Status code distribution:
  [0]	300 responses
```

优化后：

```cmd
[root@hecs-x-large-2-linux-20210831100334 ~]# ./kimbench benchmark user -c 300 -t 100

Summary:
  Total:	0.1981 secs
  Slowest:	0.1177 secs
  Fastest:	0.0266 secs
  Average:	0.0567 secs      <--- 对比
  Requests/sec:	1509.1712        <--- 对比

Response time histogram:
  0.027 [1]	|
  0.049 [112]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.072 [138]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.095 [41]	|■■■■■■■■■■■■
  0.118 [7]	|■■

Latency distribution:
  10% in 0.0364 secs
  50% in 0.0549 secs
  75% in 0.0676 secs
  90% in 0.0813 secs
  99% in 0.1101 secs

Status code distribution:
  [0]	299 responses
```

通过基准测试可以发现，虽然局部的优化性能提高了2倍左右，但是整体的性能提高没有这么大。不过这个优化的代价比较小，达到这个效果还是非常不错的。

## 最后总结

本章介绍了两个重要知识点：

1. 读、写缓冲的原理。
2. 缓冲区的复用。

通过以上两个知识点，读者应该对bufio.Reader、bufio.Writer、io.ReadFull有了一定的了解。实际上项目中还有其它的优化，比如协程池的使用，不过篇幅有限就不一一介绍了，读者可以通过阅读源码来了解更多细节。

本章完！
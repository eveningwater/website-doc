# 进阶：多租户及灰度发布

<div class="image-container">
    <img src="./docs/im/images/221.png" alt="图片29-1" title="图片29-1" >
    <span class="image-title">图 29-1 </span>
</div>

## 概要设计

### 灰度简介

灰度发布 是指在黑与白之间，能够平滑过渡的一种发布方式。同时也叫金丝雀发布，起源是，矿井工人发现，金丝雀对瓦斯气体很敏感，矿工会在下井之前，先放一只金丝雀到井中，如果金丝雀不叫了，就代表瓦斯浓度高。

<div class="image-container">
    <img src="./docs/im/images/222.png" alt="图片29-2" title="图片29-2" >
    <span class="image-title">图 29-2 </span>
</div>

在一次灰度发布流程中，先发布一个新版本V2的服务，但是并不直接将流量切过来，而是对新版本进行线上测试，如果没有问题，那么可以将少量的用户流量导入到新版本上，然后再对新版本做运行状态观察，当确认新版本运行良好后，再逐步将更多V1版本更新为新版本V2，并重复以上逻辑，直到将 100% 的流量都切换到新版本上，完成灰度发布。

<div class="image-container">
    <img src="./docs/im/images/223.png" alt="图片29-3" title="图片29-3" >
    <span class="image-title">图 29-3 </span>
</div>

如果在灰度发布过程中（灰度期）发现了新版本有问题，就应该立即将流量切回老版本上，这样，就会将负面影响控制在最小范围内，通常一次新版本发布的灰度过程会持续1~5天，取决于版本大小及影响程度。

在这个过程中，涉及到如下几点：

1. 流量控制：系统要具有灵活的流量控制逻辑。比如在部署第一个V2版本之前，需要把此节点服务的流量切为0，否则在发布更新的过程中，会导致部分流量服务不可用。
2. 灰度策略：即选择那些用户在访问系统时命中新版本，总不通过通过随机方式决定，否则用户前后几次访问系统，就会得到不同的反馈。因此，通常最简单的方式就是通过IP决定流量去向，其次可以根据地理位置、自定义Tag等策略。
3. 系统监控： 在一次新版本灰度过程中，bug不一定在自测时发现，而是被线上的某个条件触发，因此就需要有效的监控方式，相关的服务性能指标、业务故障指标等监控及报警机制是否完善，当发现异常时能及时定位问题并回滚。

当然，以上只是一个最简单的灰度发布逻辑。在实际的业务架构中，一次新版本的变更通常会涉及到调用链路上多个服务的发版，会相对更复杂。

### 多租户简介

多租户简单来说是指一个单独的实例可以为多个组织服务。多租户技术为共用的数据中心内如何以单一系统架构与服务提供多数客户端相同甚至可定制化的服务，并且仍然可以保障客户的数据隔离。一个支持多租户技术的系统需要在设计上对它的数据和配置进行虚拟分区，从而使系统的每个租户或称组织都能够使用一个单独的系统实例，并且每个租户都可以根据自己的需求对租用的系统实例进行个性化配置。

按部署模式来说，多租户又有如下情况：

1. 单独或共享应用程序
2. 单独或共享数据库

以单独的应用程序及共享数据库为例，如下图所示：

<div class="image-container">
    <img src="./docs/im/images/224.png" alt="图片29-4" title="图片29-4" >
    <span class="image-title">图 29-4 </span>
</div>

无论是共享或独享，都有各自的优缺点。多租户技术可以实现多个租户之间共享系统实例，同时又可以实现租户的系统实例的个性化定制。通过使用多租户技术可以保证系统共性的部分被共享，个性的部分被单独隔离。通过在多个租户之间的资源复用，运营管理维护资源，有效节省开发应用的成本。而且，在租户之间共享应用程序的单个实例，可以实现当应用程序升级时，所有租户可以同时升级。同时，因为多个租户共享一份系统的核心代码，因此当系统升级时，只需要升级相同的核心代码即可。

而在本章中，我们将采用共享的应用程序及数据库模式，并使用app标识从逻辑上区分出不同的租户。同时，也希望可以针对一些活跃用户非常多的大客户App，设计一个种隔离逻辑，防止意外情况下高峰流量对其它中小流量的App产生影响。

### 灰度设计

在无状态的http服务中，灰度控制的逻辑可以在SLB层或网关层处理。而在kim中，灰度控制逻辑也是在长连网关中实现。而且网关基本上没有业务逻辑，很少升级，因此非常稳定。后端业务逻辑主要是在逻辑服务（chat、login）与RPC服务（royal）中实现，因此在设计灰度逻辑时，主要考虑逻辑服务与RPC服务的流量控制。

首先，我们确定两个控制粒度：

1.灰度策略粒度： 账号、租户。
2. 流量配置粒度： 节点、集群。

由于即时通讯系统的特殊性，在灰度策略设计时，很难像无状态的web服务一样，可以根据IP、位置、账号等简单的策略去控制灰度流量。以下图为例：

<div class="image-container">
    <img src="./docs/im/images/225.png" alt="图片29-5" title="图片29-5" >
    <span class="image-title">图 29-5 </span>
</div>

当user2收到一个新格式的消息时，如果此消息格式不兼容旧版本，或者本身就是一个新的协议，那么就会存在逻辑不一致，产生不确定性。因此，从业务上来说相互隔离的多租户标识App就是一个非常理想的灰度策略。不过由此产生了另一个问题，由于APP的粒度非常大，如果在分配流量时，如果以节点为单位，就会出现严重的不均衡，如下：

<div class="image-container">
    <img src="./docs/im/images/226.png" alt="图片29-6" title="图片29-6" >
    <span class="image-title">图 29-6 </span>
</div>

而且受限于节点的负载能力，消息转发能力很容易达到系统上限。因此，如果按租户app作为灰度时的策略，就不能使用节点来承接流量。此时，可以通过把多个节点聚合成一个集群，由集群中的所有节点共同处理分流过来的指令。为了与通常意义的集群产生歧义，统称这类集群为zone 分区。如下图所示：

<div class="image-container">
    <img src="./docs/im/images/227.png" alt="图片29-7" title="图片29-7" >
    <span class="image-title">图 29-7 </span>
</div>

而且分区的另一个好处就是可以简化配置逻辑。比如，如果按服务节点为粒度管理流量，一个简化的流量配置可能如下所示：

```ini
node_A weight = 2
node_B weight = 2
node_C weight = 2
node_D weight = 2
```

如果节点有成百上千个，配置起来就会很复杂。但是分成多个区之后，粒度变大，配置就会简单很对。比如分为三个区：

```ini
zone_ali_01 weight = 3
zone_ali_02 weight = 2
zone_ali_03 weight = 5
```

无论一个分区中的服务节点有多少，由于它们的流量是平均分配，可以通过算法自动处理，无需配置，因此实际上的配置只有上面的三条。如下图：

<div class="image-container">
    <img src="./docs/im/images/228.png" alt="图片29-8" title="图片29-8" >
    <span class="image-title">图 29-8 </span>
</div>

接下来，我们直接进入实战部分 ~

## 项目实战

### 规则配置

根据以上概要设计的逻辑，可以提取几个关键信息，如下：

* zone: 流量管理最小单位。
* app: 可以作为流量分配的一种方式。
* whitelist：白名单，强制某个app被路由到指定的zone。

那么，一个流量控制的配置可以设计成如下样子：

```json
// services/gateway/route.json
{
    "routeBy": "app",
    "zones": [
        {
            "id": "zone_ali_01",
            "weight": 80
        },
        {
            "id": "zone_ali_02",
            "weight": 10
        },
        {
            "id": "zone_ali_03",
            "weight": 10
        }
    ],
    "whitelist": [
        {
            "key": "kim",
            "value": "zone_ali_03"
        }
    ]
}
```

它确定的三个主要内容：

* routeBy：灰度规则，可选 app、account。
* zones: 带权重的分区列表。
* whitelist：白名单，key为app，value为zone。

在灰度发布时，只需要调整zones的权重即可。比如可以把某个zone的weight设置为0，然后通过白名单机制，把一个测试的app路由到此zone中，这样就可以实现生产环境下的测试的同时，而不影响其它app。如果在royal层实际了app多租户数据分库逻辑，这样测试时生产的脏数据就可以路由到一个测试库，清理时也是非常方便的。

### 配置管理

在网关中，通过读取配置文件route.json，生成路由规则。如下：

```go
// services/gateway/conf/route.go
type Zone struct {
	ID     string
	Weight int
}

type Route struct {
	RouteBy   string
	Zones     []Zone
	Whitelist map[string]string
	Slots     []int
}

func ReadRoute(path string) (*Route, error) {
	var conf struct {
		RouteBy   string `json:"route_by,omitempty"`
		Zones     []Zone `json:"zones,omitempty"`
		Whitelist []struct {
			Key   string `json:"key,omitempty"`
			Value string `json:"value,omitempty"`
		} `json:"whitelist,omitempty"`
	}

	bts, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	err = json.Unmarshal(bts, &conf)
	if err != nil {
		return nil, err
	}

	var rt = Route{
		RouteBy:   conf.RouteBy,
		Zones:     conf.Zones,
		Whitelist: make(map[string]string, len(conf.Whitelist)),
		Slots:     make([]int, 0),
	}
	// build slots
	for i, zone := range conf.Zones {
		// 1.通过权重生成分片中的slots
		shard := make([]int, zone.Weight)
		// 2. 给当前slots设置值，指向索引i
		for j := 0; j < zone.Weight; j++ {
			shard[j] = i
		}
		// 3. 追加到Slots中
		rt.Slots = append(rt.Slots, shard...)
	}
	for _, wl := range conf.Whitelist {
		rt.Whitelist[wl.Key] = wl.Value
	}
	logrus.Infoln(rt)
	return &rt, nil
}
```

通过权重生成Slots的逻辑在智能路由章节已经介绍过多次了，这里就不重复了，提前生成slots是为了提高路由逻辑的性能。

注意：这里只是为了介绍逻辑，在实际情况下，这个配置还是要存放到配置中心，统一远程管理。

### 实现Selector

在前面章节中，我们介绍过，消息的转发是在container包中完成的，而其中核心的路由规则就是HashSelector，再次回顾下其中的调用逻辑。如下：

```go
//container/container.go
package container

func lookup(serviceName string, header *pkt.Header, selector Selector) (kim.Client, error) {
	clients, ok := c.srvclients[serviceName]
	if !ok {
		return nil, fmt.Errorf("service %s not found", serviceName)
	}
	// 只获取状态为StateAdult的服务
	srvs := clients.Services(KeyServiceState, StateAdult)
	if len(srvs) == 0 {
		return nil, fmt.Errorf("no services found for %s", serviceName)
	}
	id := selector.Lookup(header, srvs)     <-- 选择一个srv
	if cli, ok := clients.Get(id); ok {
		return cli, nil
	}
	return nil, fmt.Errorf("no client found")
}
```

而这个selector是可以被修改的。因此，在网关中，我们可以创建一个定义的selector，并把它注入到container中。如下：

```go
//services/gateway/server.go

func RunServerStart(ctx context.Context, opts *ServerStartOptions, version string) error {
        ...
        // use routeSelector
	selector, err := serv.NewRouteSelector(opts.route)
	if err != nil {
		return err
	}
	container.SetSelector(selector)
	return container.Start()
}
```

因此，流量控制的核心逻辑就落在了RouteSelector中。我们重点介绍它的实现逻辑。代码如下：

```go
//services/gateway/serv/selector.go
package serv

import (
	"hash/crc32"
	"math/rand"

	"github.com/klintcheng/kim"
	"github.com/klintcheng/kim/logger"
	"github.com/klintcheng/kim/services/gateway/conf"
	"github.com/klintcheng/kim/wire/pkt"
)

// RouteSelector RouteSelector
type RouteSelector struct {
	route *conf.Route
}

func NewRouteSelector(configPath string) (*RouteSelector, error) {
	route, err := conf.ReadRoute(configPath)
	if err != nil {
		return nil, err
	}
	return &RouteSelector{
		route: route,
	}, nil
}

// Lookup a server
func (s *RouteSelector) Lookup(header *pkt.Header, srvs []kim.Service) string {
	// 1. 从header中读取Meta信息
	app, _ := pkt.FindMeta(header.Meta, MetaKeyApp)
	account, _ := pkt.FindMeta(header.Meta, MetaKeyAccount)
	if app == nil || account == nil {
		ri := rand.Intn(len(srvs))
		return srvs[ri].ServiceID()
	}
	log := logger.WithFields(logger.Fields{
		"app":     app,
		"account": account,
	})

	// 2. 判断是否命中白名单
	zone, ok := s.route.Whitelist[app.(string)]
	if !ok { // 未命中情况
		var key string
		switch s.route.RouteBy {
		case MetaKeyApp:
			key = app.(string)
		case MetaKeyAccount:
			key = account.(string)
		default:
			key = account.(string)
		}
		// 3. 通过权重计算出zone
		slot := hashcode(key) % len(s.route.Slots)
		i := s.route.Slots[slot]
		zone = s.route.Zones[i].ID
	} else {
		log.Infoln("hit a zone in whitelist", zone)
	}
	// 4. 过滤出当前zone的servers
	zoneSrvs := filterSrvs(srvs, zone)
	if len(zoneSrvs) == 0 {
		log.Warnf("select a random service from all due to no service found in zone %s", zone)
		ri := rand.Intn(len(srvs))
		return srvs[ri].ServiceID()
	}
	// 5. 从zoneSrvs中选中一个服务
	srv := selectSrvs(zoneSrvs, account.(string))
	return srv.ServiceID()
}
```

在Lookup方法中，主要有5个核心逻辑：

1.从header中读取Meta信息。

首先就是从消息header中读取出App、Account信息。不过到目前为止，消息Header中是没有这些信息的。在网关中，只有在登录时，才可以从token中得到app及account信息，因此需要把这个信息保存，并在接收消息时注入，完整的链路如下：

<div class="image-container">
    <img src="./docs/im/images/229.png" alt="图片29-9" title="图片29-9" >
    <span class="image-title">图 29-9 </span>
</div>

要完成这个逻辑，需要修改以上链路中的代码，详细逻辑读者可以查看源码。这里主要看看在Receive方法中注入Meta的逻辑。如下：

```go
//services/gateway/serv/handler.go
func (h *Handler) Receive(ag kim.Agent, payload []byte) {
	if logicPkt, ok := packet.(*pkt.LogicPkt); ok {
		logicPkt.ChannelId = ag.ID()
		// 把meta注入到header中
		if ag.GetMeta() != nil {
			logicPkt.AddStringMeta(MetaKeyApp, ag.GetMeta()[MetaKeyApp])
			logicPkt.AddStringMeta(MetaKeyAccount, ag.GetMeta()[MetaKeyAccount])
		}
		err = container.Forward(logicPkt.ServiceName(), logicPkt)
	}
}
```

2.判断是否命中白名单：目前只设计了app白名单机制，如果命中，就可以跳过选择zone的逻辑。

3.通过权重计算出zone：这里就是一个通过前面计算的slots，计算出命中的zone，如果RouteBy为app时，由于不同app的在线用户是不相同的，因此每个分区中的并发量实际上并不是这个权重的比值，只有当RouteBy为account时，每个分区实际的并发流量才会接近这个配置

4.过滤出当前zone的servers：这里就是一个简单的选择逻辑，在chat服务把自己注册到注册中心时，就需要给自己标记一个zone信息。

```go
//services/gateway/serv/selector.go
func filterSrvs(srvs []kim.Service, zone string) []kim.Service {
	var res = make([]kim.Service, 0, len(srvs))
	for _, srv := range srvs {
		if zone == srv.GetMeta()["zone"] {
			res = append(res, srv)
		}
	}
	return res
}
```

5.从zoneSrvs中选中一个服务：利用权重计算出的slots，再次通过一致性Hash得到service。

```go
func selectSrvs(srvs []kim.Service, account string) kim.Service {
	slots := make([]int, 0, len(srvs)*10)
	for i := range srvs {
		for j := 0; j < 10; j++ {
			slots = append(slots, i)
		}
	}
	slot := hashcode(account) % len(slots)
	return srvs[slots[slot]]
}
```

需要说明的是，由于Lookup不允许返回空值，否则消息就不知道要发送到哪个server中了，因此这里做了一个容错机制。比如在配置异常，或者某个zone中的服务宕机时，就会进入默认的随机选择器。

```go
ri := rand.Intn(len(srvs))
return srvs[ri].ServiceID()
```

## 最后总结

本章节从即时通讯的角度介绍了灰度发布的一种实现方式，在未实现完整的灰度发布流程之前，通常一次发布的风险会比较大，而且会导致一段时间系统不可用，因此通常需要在活跃用户较少的时间段发布，所以一套灵活可靠的灰度发布系统及流程是非常重要的。同时，分区逻辑对royal这类无状态的服务来说，规则及路由方式也是通用的，只需要做一些小的改动，就可以完成整个链路上的灰度发布的闭环，在这里就当抛砖引玉，读者可以思考下有那些实现方式。

最后，介绍完灰度发布及多租户，小册涉及的相关功能就告一段落，下一章节将重点介绍系统监控相关的知识点。

本章完！